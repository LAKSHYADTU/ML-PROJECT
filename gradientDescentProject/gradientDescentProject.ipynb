{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "plt.rcParams['figure.figsize'] = [4, 3] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GRADIENT DESCENT :**\n",
    "* is a first-order iterative optimization algorithm for finding local minimum of a desirable funcn by taking repeated steps in opposite direcn of gradient / slope of funcn at current point.\n",
    "* As we start moving closer to the soln we take smaller jumps because of decr in slope , Also it ensured we never miss our soln.\n",
    "\n",
    "**WHEN TO STOP IN GRADIENT DESCENT :**\n",
    "* when diff abs(bnew - bold) < 0.0001 .\n",
    "* When no. of epochs completed.\n",
    "\n",
    "**CALCULATING REGRESSION COEFFICIENT , INTERCEPT & R2 SCORE BY INBUILT FUNCTION IN ADVANCE :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
      "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
      "151.88331005254167\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4399338661568969\n",
      "(353, 10)\n"
     ]
    }
   ],
   "source": [
    "y_pred = reg.predict(X_test)\n",
    "print(r2_score(y_test,y_pred))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **COMPARING DIFFERENT GRADIENT DESCENT BY IMPLEMENTING CODE FOR EACH GRADIENT FROM SCRATCH :**\n",
    "### **1) BATCH GRADIENT DESCENT :**\n",
    "\n",
    "**BATCH GRADIENT DESCENT CODE FOR MULTIPLE LINEAR REGRESSION :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
    "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
    "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "            \n",
    "            coef_der = -2 * np.dot((y_train - y_hat).T,X_train)/X_train.shape[0]\n",
    "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "            \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151.94042847773682 [  62.27835432  -24.14017912  262.40285385  192.20751489   39.48809013\n",
      "   10.26886323 -142.50597903  124.33312557  244.33510843  119.34350233]\n",
      "0.3971698388048742\n"
     ]
    }
   ],
   "source": [
    "gdr = GDRegressor(epochs=1000,learning_rate=0.1)\n",
    "gdr.fit(X_train,y_train)\n",
    "y_pred = gdr.predict(X_test)    \n",
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"batchGradientDescent.png\" style=\"height:500px; width:1000px; margin-left:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BATCH GRADIENT DESCENT REQUIRE : LEARNING RATE = 0.1 , EPOCHS = 1000**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2) STOCHASTIC GRADIENT DESCENT :**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDRegressor:\n",
    "    \n",
    "    def __init__(self,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            for j in range(X_train.shape[0]):\n",
    "                idx = np.random.randint(0,X_train.shape[0])\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                \n",
    "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "                \n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(learning_rate=0.03,epochs=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.0227118779365 [   8.50107428 -156.34353759  483.03109706  312.49989965  -32.81440276\n",
      " -116.80650007 -197.74878407  102.81978907  454.07138232   94.07954394]\n",
      "0.4093797990174697\n"
     ]
    }
   ],
   "source": [
    "sgd.fit(X_train,y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(r2_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stochasticGradientDescent.png\" style=\"height:500px; width:1000px; margin-left:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STOCHASTIC GRADIENT REQUIRE : LEARNING RATE = 0.03 , EPOCHS = 45**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3) MINI BATCH GRADIENT DESCENT :**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MBGDRegressor:\n",
    "    \n",
    "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
    "        \n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def fit(self,X_train,y_train):\n",
    "        # init your coefs\n",
    "        self.intercept_ = 0\n",
    "        self.coef_ = np.ones(X_train.shape[1])\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
    "                \n",
    "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
    "                \n",
    "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
    "                #print(\"Shape of y_hat\",y_hat.shape)\n",
    "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
    "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
    "\n",
    "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
    "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
    "        \n",
    "        print(self.intercept_,self.coef_)\n",
    "    \n",
    "    def predict(self,X_test):\n",
    "        return np.dot(X_test,self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbr = MBGDRegressor(batch_size=int(X_train.shape[0]/50),learning_rate=0.02,epochs=121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.65558866633862 [  -1.53078376 -206.04415293  525.11813731  344.13116263  -47.12059255\n",
      " -126.7834178  -183.14848485   87.75327898  502.72947703   70.24269703]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4471218073329646"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbr.fit(X_train,y_train)\n",
    "y_pred = mbr.predict(X_test)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"miniBatchGradientDescent.png\" style=\"height:500px; width:1000px; margin-left:50px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MINI BATCH GRADIENT REQUIRE : LEARNING RATE = 0.02 , EPOCHS = 121**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
